\documentclass[runningheads]{IEEEtran}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{authblk}
\pagestyle{empty}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Optimizing ImageBind: Enhancing Multimodal Embedding Models}
\author{}
\maketitle

% \author[1]{Ahmed Saed}
% \author[2]{Adham Essam}
% \author[3]{Ahmed Maher}
% \author[4]{Mohamed Ayman}
% \author[5]{Mohamed Sherbiny}
% \author[6]{Alaaeldin Abdallah}
% \author[7]{Ahmed Tarek}
% \affil[1]{mail@ahmedsaed.me}
% \affil[2]{adhamessammetwally@gmail.com}
% \affil[3]{ahmaher04@gmail.com}
% \affil[4]{mohamed.ayman.ae@gmail.com}
% \affil[5]{mohammed.ys2003@gmail.com}
% \affil[6]{Aboelmnzer@gmail.com}
% \affil[6]{Alaaeldin.yassin@must.edu.eg}
% \affil[7]{ahmed.elashry@must.edu.eg}

\begin{abstract}
ImageBind has proven to be a powerful model that generates unified embeddings across different modalities, providing a great approach to cross-modal understanding. In this work, we experiment with architectural and post-training optimizations for improving the resource usage and ease of use of ImageBind. We purpose a modular approach that separates the model into independent, modality-specific submodels, allowing for on-demand loading and inference based on the modalities relevant to a given task. In addition, we explore model quantization, experimenting with both dynamic and static quantization techniques. These optimizations lead to a notable decrease in memory usage and inference time with minimal impact to the integrity of multimodal embeddings. We also investigate compatibility with edge devices and multi-threaded inference setups, highlighting how our optimizations maintain performance while expanding deployment options, enabling real-time applications such as AR, robotics, and assistive tech. Thus, making ImageBind more scalable and applicable in real-world scenarios where resources are constrained.
\end{abstract}

\begin{IEEEkeywords}
ImageBind, Embeddings, Optimizations, Quantization, Decomposition
\end{IEEEkeywords}


\section{Introduction}

ImageBind \cite{girdhar2023imagebindembeddingspacebind} represents a novel contribution in multimodal representation learning by learning a unified embedding space across six modalities: images, text, audio, depth, thermal, and inertial measurement unit (IMU) data. Its innovation lies in bringing all these modalities together based on just image-paired data—successfully removing the requirement of large co-occurring multimodal datasets. By leveraging the innate connections that images have to other sensory inputs, ImageBind is able to learn unified representations capable of facilitating powerful zero-shot performance across a wide range of modalities. Thus enabling functionalities like cross-modal retrieval tasks, compositional reasoning via embedding arithmetic, in addition to detection and generation tasks across diverse modalities.

The work presented herein purposes a series of enhancements for improving the efficiency, modularity, and overall usability of ImageBind without compromising its robust capabilities for cross-modal representation. As an initial step, we restructured the architecture into discrete, modality-specific components, enabling selective inference based on the modalities relevant to the task. Additionally, we investigated quantization, both dynamic and static techniques, where we significantly reduced the model’s memory footprint with minimal degradation in performance. Collectively, these enhancements make ImageBind more flexible and efficient for a wide range of real-world applications where resources are constrained.

\section{Related Works}
The swift advancement of multimodal embedding models has been driven by the success of contrastive learning models, including CLIP [Radford et al., 2021] \cite{radford2021learningtransferablevisualmodels}, ALIGN [Jia et al., 2021] \cite{jia2021scalingvisualvisionlanguagerepresentation}, and Florence [Yuan et al., 2021]\cite{yuan2021florencenewfoundationmodel}. These models have shown that big image-text alignment can be attained effectively using paired datasets and scalable architectures. AudioCLIP [Guzhov et al., 2021]\cite{guzhov2021audioclipextendingclipimage} expanded this to the audio modality, and ImageBind [Girdhar et al., 2023]\cite{girdhar2023imagebindembeddingspacebind} expanded upon this by jointly embedding six distinct modalities within one representational space at once. In spite of their state-of-the-art performance, increasing modality coverage tends to result in much larger model sizes.

The inherently modular structure of models such as CLIP and ImageBind opens up promising avenues for optimization. These models rely on separate modality-specific encoders, allowing for targeted improvements such as weight pruning, quantization, and selective activation, without disrupting the entire architecture.

In model compression, Bondarenko et al. [2021] tackled the challenges of efficient transformer quantization\cite{bondarenko2021understandingovercomingchallengesefficient}, a key problem because transformer-based backbones are the foundation for numerous such multimodal models. Their work is to highlight trade-offs between latency and precision, with an emphasis on the difficulty of maintaining performance when compressing the model.

Likewise, Zhu and Gupta [2017] examined pruning methods in their work "To Prune or Not to Prune"\cite{zhu2017prunepruneexploringefficacy}, demonstrating that big sparse models can achieve better results than the same-sized small dense models in most tasks. Their incremental pruning method provides an effective means of compressing deep neural networks to be implemented on resource-limited environments, and their results strongly speak in favor of the importance of hardware accelerators specialized in sparse computation.

Taking advantage of pruning, Kwon et al. [2022] proposed a post-training pruning framework for transformers\cite{kwon2022fastposttrainingpruningframework} that achieved up to 50\% FLOPs reduction and latency savings with negligible loss in accuracy—all in a matter of seconds of computation. This demonstrates the feasibility of quick, training-free model optimization.

A complementary approach is knowledge distillation. Gou et al. [2021] provided a thorough review of this approach \cite{Gou_2021}, in which a smaller "student" model learns to imitate a larger pretrained "teacher." This strategy enables substantial size and inference cost reductions with much of the original performance preserved—a central concern for real-time or edge deployment of multimodal models.

Our work builds upon these prior efforts, providing a strong foundation for exploring optimizations for multimodal embedding models like ImageBind in resource-constrained environments.

\section{Methodology and Experimental Setup}

We propose a modular architecture for the ImageBind model to reduce its memory footprint during inference, especially when working with a subset of modalities. Our approach decomposes the monolithic architecture into independent submodules per modality, allowing selective loading and inference. In addition to modularization, we explore model quantization techniques, both dynamic and static, to further compress the model and accelerate inference while maintaining reasonable performance across modalities.

\subsection{Modular Decomposition}

The original ImageBind model, released as a single monolithic architecture, includes multiple modality pipelines bundled together which is memory inefficient when only a subset of modalities is required. To overcome this limitaion, we split the model into individual submodules, one for each of the supported modalities: vision, text, audio, depth, IMU, and thermal. Each submodule includes its own preprocessor, trunk, head, and postprocessor. The proposed architectural restructuring enables loading only the necessary components at inference, significantly reducing the GPU memory footprint. The resulting sizes of the modular weights are summarized in Table ~\ref{table:weights}.

\begin{table}[htbp]
\caption{Per-Modality Checkpoint Sizes After Splitting}
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Modality} & \textbf{Checkpoint Size} \\
\hline
Audio & 329.018 MB \\
Depth & 83.490 MB \\
IMU & 74.798 MB \\
Text & 1.318 GB \\
Thermal & 328.925 MB \\
Vision & 2.357 GB \\
\hline
\textbf{Total (Combined)} & \textbf{4.473 GB} \\
\hline
\end{tabular}
\label{table:weights}
\end{center}
\end{table}

To facilitate modular execution, we extended the model loading mechanism to accept a list of desired modalities. Thus only the relevant submodules are instantiated and loaded into memory. This effectively enables the model to operate in a reduced configuration without impacting other modality pipelines or the embedding performance.

% \subsubsection{Memory Profiling and Validation}

We validated the memory optimization using several tools and techniques:

\begin{itemize}
    \item \textbf{Python's \texttt{memory\_profiler}} was employed to monitor peak memory usage during inference with different modality subsets.
    \item \textbf{Manual calculation} of model memory requirements was performed using the parameter and buffer counts per submodule.
    \item \textbf{Runtime diagnostics} were conducted to ensure that unnecessary submodules were not inadvertently loaded.
\end{itemize}


\subsection{Quantization of ImageBind}

Quantization is an effective technique for reducing the memory footprint and computational load of neural networks by transforming floating-point weights and activations into lower-bit representations. This section presents our experimental results on model quantization applied to ImageBind. We explored both dynamic and static quantization approaches using PyTorch's Eager Mode Quantization framework.

As of PyTorch 2.6, there are three quantization methods available: Eager Mode Quantization (beta), FX Graph Mode Quantization (prototype), and PyTorch 2 Export Quantization (prototype). Due to its more mature implementation, we selected Eager Mode for our experiments, focusing specifically on dynamic and static quantization techniques.

\sloppypar{
The ImageBind architecture incorporates diverse layer types including \texttt{Conv2d}, \texttt{Conv3d}, \texttt{Linear}, \texttt{Embedding}, and \texttt{MultiheadAttention} layers. Different quantization modes offer varying levels of support for these layers. Dynamic quantization operates at inference time, converting weights to lower precision while keeping activations in full precision. Static quantization quantizes both weights and activations using a calibration process on representative data.
}

\subsubsection{Dynamic Quantization}
offers an accessible approach that does not require architectural modifications. We applied PyTorch's \texttt{quantize\_dynamic} method with the following configuration:

\sloppypar{
\begin{itemize}
    \item \texttt{qconfig\_spec}: Targeted \texttt{nn.Linear}, \texttt{nn.LayerNorm}, \texttt{nn.Embedding}, \texttt{nn.Dropout}, and \texttt{nn.GELU} layers
    \item \texttt{dtype}: \texttt{int8} precision for weights
\end{itemize}

For \texttt{Embedding} layers, we set the quantization configuration to \texttt{float\_qparams\_weight\_only\_qconfig}. We evaluated the quantized model using a combination of representative examples and random inputs across all modalities. Our results demonstrate significant improvements in model efficiency summarized in Table \ref{tab:dynamic_quant_all} and
Table~\ref{tab:dynamic_quant_cosine} shows the cosine similarity between outputs from the original and quantized models across different modalities.
}

\begin{table}[ht]
\caption{Dynamic Quantization Performance (All Supported Layers)}
\label{tab:dynamic_quant_all}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Original model size & 4581.14 MB \\
Quantized model size & 2246.47 MB \\
Size reduction & 50.96\% \\
Speed improvement & 33.20\% \\
\hline
\end{tabular}
\end{table}


\begin{table}[ht]
\caption{Cosine Similarity Between Original and Dynamically Quantized Model Outputs}
\label{tab:dynamic_quant_cosine}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Modality} & \textbf{Average Cosine Similarity} \\
\hline
Vision & 0.943 \\
Text & 0.906 \\
Audio & 0.989 \\
Depth & 0.993 \\
Thermal & 0.999 \\
IMU & 0.999 \\
\hline
\end{tabular}
\end{table}

Since \texttt{Linear} layers account for the majority of the model's parameters, we conducted an additional experiment quantizing only these layers. The results were comparable to quantizing all supported layers, as shown in Tables~\ref{tab:dynamic_quant_linear} and \ref{tab:dynamic_quant_linear_cosine}.

\begin{table}[ht]
\caption{Dynamic Quantization Performance (Linear Layers Only)}
\label{tab:dynamic_quant_linear}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Original model size & 4581.14 MB \\
Quantized model size & 2246.47 MB \\
Size reduction & 50.96\% \\
Speed improvement & 32.93\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Cosine Similarity for Linear-Only Quantization}
\label{tab:dynamic_quant_linear_cosine}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Modality} & \textbf{Average Cosine Similarity} \\
\hline
Vision & 0.948 \\
Text & 0.898 \\
Audio & 0.990 \\
Depth & 0.993 \\
Thermal & 0.999 \\
IMU & 0.999 \\
\hline
\end{tabular}
\end{table}

These results demonstrate that dynamic quantization achieves substantial efficiency gains while maintaining output similarity above 90\% across all modalities.

\subsubsection{Static Quantization}

offers a more comprehensive optimization by quantizing both weights and activations, resulting in greater size reduction and performance improvements. However, it requires architectural modifications to support the quantization process.

ImageBind's architecture consists of four module types for each modality: preprocessor, trunk, head, and postprocessor. We excluded preprocessors from quantization due to their relatively small parameter count and critical impact on downstream modules. The following modifications were necessary to support static quantization:

\sloppypar{
\begin{enumerate}
    \item Addition of \texttt{QuantStub} modules at input points to capture and quantize input tokens from each modality
    \item Custom handling of the \texttt{Normalize} module, which uses \texttt{torch.nn.functional.normalize} (not directly quantizable)
    \item Integration of \texttt{QuantStub} and \texttt{DeQuantStub} wrappers around the \texttt{DropPath} module from the TIMM library
    \item Custom mapping for \texttt{MultiheadAttention} modules using \texttt{custom\_module\_config} to their quantizable counterparts
    \item Addition of a final \texttt{DeQuantStub} to convert output embeddings back to float32
\end{enumerate}
}

These modifications maintain the original architecture's functionality while enabling quantization support. Our static quantization process followed three key steps:

\begin{enumerate}
    \item \textbf{Preparation}: Attaching observer or fake quantization modules and propagating quantization configurations
    \item \textbf{Calibration}: Passing representative data through the model for observers to analyze tensor value distributions
    \item \textbf{Conversion}: Converting modules to their quantized versions and removing observer modules
\end{enumerate}

\sloppypar{
We configured quantization parameters using PyTorch's \verb|QConfig| with \verb|HistogramObserver| for activations (\verb|quant_max=255|, \verb|quant_min=0|) and \verb|default_per_channel_weight_observer| for weights. Results are presented in Tables~\ref{tab:static_quant} and \ref{tab:static_quant_cosine}.
}


\begin{table}[ht]
\caption{Static Quantization Performance}
\label{tab:static_quant}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Original model size & 4581.14 MB \\
Quantized model size & 1316.30 MB \\
Size reduction & 71.27\% \\
Speed improvement & 68.96\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Cosine Similarity Between Original and Statically Quantized Model Outputs}
\label{tab:static_quant_cosine}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Modality} & \textbf{Average Cosine Similarity} \\
\hline
Vision & 0.779 \\
Text & 0.407 \\
Audio & 0.929 \\
Depth & 0.979 \\
Thermal & 0.985 \\
IMU & 0.998 \\
\hline
\end{tabular}
\end{table}


Our full implementation, including training scripts and model configurations, is available at \cite{saed2025_modular_pr} and \cite{saed2025_quantization_branch}

\section{Analysis and Discussion}

The modularization experiment demonstrated that decomposing the ImageBind model into modality-specific submodules enabled selective loading which reduced memory usage significantly during inference. The observed memory usage during inference was in line with the expected theoretical values calculated from the sizes of the loaded checkpoints. For example, using only the vision and text modalities resulted in approximately 3764 MB of memory usage, which matched the combined sizes of the corresponding submodules. 

Our quantization experiments revealed significant differences between dynamic and static quantization approaches. Dynamic quantization achieved a 50.96\% size reduction with 33.20\% speed improvement while maintaining high output similarity ($>$90\%) across all modalities. Static quantization delivered superior efficiency gains with 71.27\% size reduction and 68.96\% speed improvement but exhibits substantial accuracy degradation in text (40.7\%) and vision (77.9\%) modalities.

Several factors may contribute to the accuracy degradation in static quantization:

\begin{itemize}
    \item \textbf{Calibration data quality}: Our proof-of-concept used random inputs rather than truly representative data, potentially leading to suboptimal quantization parameters.
    \item \textbf{Quantization granularity}: Text and vision modalities may require more nuanced per-layer quantization strategies or selective quantization of critical layers.
    \item \textbf{Architecture sensitivity}: These modalities might incorporate operations particularly sensitive to activation quantization.
\end{itemize}

Future work should explore improved calibration techniques with domain-specific data, per-layer quantization configurations, and quantization-aware training to mitigate accuracy loss.

\section{Conclusion}

We proposed and evaluated two optimization techniques for the ImageBind model: modularization and quantization. Our modularization approach successfully reduced memory overhead by allowing selective loading of sub-modules based on required modalities. Memory usage during inference was consistent with theoretical expectations,thereby  confirming the feasibility of fine-grained modular deployment. 

Quantization experiments also reduced resource requirements. Dynamic quantization provided a good balance between model size, speed, and accuracy, making it a practical solution for real-world applications. Although static quantization offered more substantial efficiency improvements, it introduced unacceptable performance degradation in certain modalities. Addressing these issues through better calibration and quantization-aware training remains a promising avenue for future research.

Together, these results highlight the potential of modular, quantized multi-modal embedding models for efficient, inference in resource-constrained settings.

\bibliographystyle{IEEEtran}
\bibliography{refrences}

\end{document}


